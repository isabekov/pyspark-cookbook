#+AUTHOR: Altynbek Isabekov
#+EMAIL: aisabekov@ku.edu.tr
#+LANGUAGE: en
#+PROPERTY: header-args:emacs-lisp :results silent
#+PROPERTY: header-args:python :noweb strip-export :exports both :results output raw drawer replace :session pyspark
#+SETUPFILE: src/setupfile_html.org
* To convert Pandas dataframe to tabular format using tabulate()
#+header: :prologue from tabulate import tabulate
#+header: :noweb strip-export
#+begin_src python :results output raw
  import pandas as pd
  df = pd.DataFrame({
      "a": [1,2,3],
      "b": [4,5,6]
  })
  df_str = tabulate(df, headers=df.columns, tablefmt="orgtbl", showindex=False)
  print(df_str)
#+end_src

#+RESULTS:
| a | b |
|---+---|
| 1 | 4 |
| 2 | 5 |
| 3 | 6 |

* To convert Pandas dataframe to tabular format using pd2org()
#+name: pd2org
#+begin_src python :var df="df" :exports none :session none
  return f"return tabulate({df}, headers={df}.columns, tablefmt='orgtbl', showindex=False)"
#+end_src

#+header: :prologue from tabulate import tabulate
#+header: :noweb strip-export
#+begin_src python :results value raw :noweb strip-export :session none
  import pandas as pd
  df = pd.DataFrame({
      "a": [1,2,3],
      "b": [4,5,6]
  })
  <<pd2org("df")>>
#+end_src

#+RESULTS:
| a | b |
|---+---|
| 1 | 4 |
| 2 | 5 |
| 3 | 6 |

* To convert PySpark dataframe to tabular format using ps2org()
#+name: ps2org
#+header: :noweb strip-export
#+begin_src python :var df_in="df_in" :exports none :session none :results value raw
  return f"return {df_in}.toPandas().to_markdown(index=False, tablefmt='orgtbl')"
#+end_src

#+header: :prologue from tabulate import tabulate
#+header: :noweb strip-export
#+BEGIN_SRC python :var df="df" :results value raw :session test-pyspark
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("a", T.IntegerType(), True),
          T.StructField("b", T.IntegerType(), True),
      ]
  )
  data = [(1, 4), (2, 5), (3, 6)]
  df = spark.createDataFrame(schema=schema, data=data)
  <<ps2org("df")>>
#+END_SRC

#+RESULTS:
| a | b |
|---+---|
| 1 | 4 |
| 2 | 5 |
| 3 | 6 |

* To convert PySpark dataframe to tabular format using actual and shown code
#+name: show2org
#+begin_src python :var df_in="df_in" :exports none :results value raw :session none
  return f"print({df_in}.toPandas().fillna('null').to_markdown(index=False, tablefmt='orgtbl') + '\\n')#"
#+end_src

#+RESULTS: show2org

#+NAME: nostderr
#+BEGIN_SRC python :var spark="spark" :exports none :results value raw
  return "spark.sparkContext.setLogLevel('OFF')"
#+END_SRC

#+RESULTS: nostderr
spark.sparkContext.setLogLevel('OFF')

#+header: :prologue from tabulate import tabulate
#+header: :noweb strip-export
#+BEGIN_SRC python :results output raw  :exports both :session test-pyspark
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.config("spark.log.level", "OFF").master("local").appName("test-app").getOrCreate()
  <<nostderr("spark")>>

  schema = T.StructType(
      [
          T.StructField("a", T.IntegerType(), True),
          T.StructField("b", T.IntegerType(), True),
      ]
  )
  data = [(1, 4), (2, 5), (3, 6)]
  df = spark.createDataFrame(schema=schema, data=data)
  print("Dataframe df:")
  <<show2org("df")>>df.show()
#+END_SRC

#+RESULTS:
|   a |   b |
|-----+-----|
|   1 |   4 |
|   2 |   5 |
|   3 |   6 |

Which is converted into the following code block during evaluation:
#+BEGIN_SRC python :results output raw  :exports both
  from tabulate import tabulate
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("a", T.IntegerType(), True),
          T.StructField("b", T.IntegerType(), True),
      ]
  )
  data = [(1, 4), (2, 5), (3, 6)]
  df = spark.createDataFrame(schema=schema, data=data)
  print(df.toPandas().to_markdown(index=False, tablefmt='orgtbl'))#df.show()
#+END_SRC
* To convert PySpark dataframe to tabular format using returned value and NOWEB
#+name: litps2org
#+begin_src python :exports none :results value raw :session none
  .toPandas().to_markdown(index=False, tablefmt='orgtbl')#
#+end_src

#+header: :noweb strip-export
#+BEGIN_SRC python :results value :exports both :session pyspark
  # Built-in namespace
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("a", T.IntegerType(), True),
          T.StructField("b", T.IntegerType(), True),
      ]
  )
  data = [(1, 4), (2, 5), (3, 6)]
  df = spark.createDataFrame(schema=schema, data=data)
  df<<litps2org>>.show()
#+END_SRC

#+RESULTS:
: |   a |   b |
: |-----+-----|
: |   1 |   4 |
: |   2 |   5 |
: |   3 |   6 |

* To convert PySpark dataframe to tabular format using post-processing with AWK
#+name: pretty2orgtbl
#+begin_src sh :var data="" :results output
  echo "$data"  | awk 'BEGIN{state_prev=""; prev_line=""}{                          \
                if ($0 ~ /^\+[-+]+\+$/){                                            \
                       state_curr = "hline"                                         \
                } else {                                                            \
                       if ($0 ~ /^\|.*\|$/) {                                       \
                            state_curr = "tblbody"                                  \
                        }                                                           \
                        else {                                                      \
                            state_curr = "txt"                                      \
                        }                                                           \
                 }                                                                  \
                                                                                    \
                if ((state_curr == "hline") && (state_prev == "txt")) {             \
                       printf("%s", prev_line);                                     \
                       prev_line = "";                                              \
                } else if ((state_curr == "txt") && (state_prev == "hline")) {      \
                       prev_line = $0;                                              \
                } else if ((state_curr == "hline") && (state_prev == "")) {         \
                       prev_line = "";                                              \
                } else if ((state_curr == "txt") && (state_prev == "")) {           \
                       printf("%s", prev_line);                                     \
                       prev_line = gensub(/^\+([-+]+)\+$/, "|\\1|", "g", $0);       \
                } else {                                                            \
                       if (NR > 2) {                                                \
                            printf("%s\n", prev_line);                              \
                       }                                                            \
                       prev_line = gensub(/^\+([-+]+)\+$/, "|\\1|", "g", $0);       \
                }                                                                   \
                state_prev = state_curr;                                            \
                }END{if (prev_line !~ /^\|.*\|$/) {print prev_line}}'
#+end_src

#+name: pyspark-table
#+header: :noweb strip-export
#+begin_src python :results output raw drawer :session pyspark :post pretty2orgtbl(data=*this*)
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  from tabulate import tabulate
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.StringType()), True),
          T.StructField("B", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [(["b", "a", "c"], ["c", "d", "a", "f"])]
  df = spark.createDataFrame(schema=schema, data=data)

  dft = df.select("A", "B",
            F.array_except("A", "B").alias("A\B"),
            F.array_except("B", "A").alias("B\A"))
  print("Table 1:")
  dft.show()

  print("Table 2:")
  dft.show()

  print("Two tables are the same.")
#+end_src

#+RESULTS: pyspark-table
:results:
Table 1:
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|

Table 2:
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|

Two tables are the same.
:end:

* To convert PySpark dataframe to tabular format using post-processing with SED
#+name: pretty2orgtbl_w_hlines
#+begin_src sh :var data="" :results output
  echo "$data" | sed -E "s/^\+([-+]+)\+$/|\1|/g"
#+end_src

#+CALL: pyspark-table() :post pretty2orgtbl_w_hlines(data=*this*)

#+RESULTS:
:results:
Table 1:
|---------+------------+---+------|
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|
|---------+------------+---+------|

Table 2:
|---------+------------+---+------|
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|
|---------+------------+---+------|

Two tables are the same.
:end:
* To convert PySpark dataframe to tabular format using post-processing with Python
The formatting of PySpark dataframe is done in [[https://github.com/apache/spark/blob/branch-3.5/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L347][.showString()]].
#+name: pretty2orgtbl_python
#+begin_src python :var data="" :exports none :results output :session none :exports none
  import re
  state_prev = ""
  prev_line = ""

  for j, line in enumerate(data.split("\n")):
      # Prepend a white space to square brackets to prevent Org-Mode
      # from recognizing PySpark arrays as links (skip real links like "[[file:...]]")
      if not re.search("file:", line):
          line = re.sub("\]\]", " ]]", line)

      if re.match("^\+[-+]+\+$", line):
          state_curr = "hline"
      elif re.match("^\|.*\|$", line):
          state_curr = "tblbody"
      else:
          state_curr = "txt"

      if (state_curr == "hline") & (state_prev == "txt"):
          print(prev_line, end="")
          prev_line = ""
      elif (state_curr == "txt") & (state_prev == "hline"):
          print("", end="")
          prev_line = line
      elif (state_curr == "txt") & (state_prev == ""):
          print(prev_line, end="")
          prev_line = re.sub("^\+([-+]+)\+$", "|\\1|", line)
      else:
          if j > 0:
              print(prev_line, end="\n")
          prev_line = re.sub("^\+([-+]+)\+$", "|\\1|", line)

      state_prev = state_curr

  if not re.match("^\|[-+]+\|$", prev_line):
      print(prev_line)
#+end_src

#+CALL: pyspark-table() :post pretty2orgtbl_python(data=*this*)

#+RESULTS:
:results:
Table 1:
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|

Table 2:
|        A|           B|A\B|   B\A|
|---------+------------+---+------|
|[b, a, c]|[c, d, a, f]|[b]|[d, f]|

Two tables are the same.
:end:
* To convert PySpark dataframe to HTML format using a built-in function
#+name: pyspark-table-repr-html
#+header: :noweb strip-export
#+begin_src python :results output html :session pyspark
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  from pyspark import SparkConf

  # This configuration is needed to enable HTML rendering
  conf = SparkConf().set("spark.sql.repl.eagerEval.enabled", "true")

  spark = SparkSession.builder.master("local[1]").appName("test-app").config(conf=conf).getOrCreate()
  schema = T.StructType(
      [
          T.StructField("a", T.IntegerType(), True),
          T.StructField("b", T.IntegerType(), True),
      ]
  )
  data = [(1, 4), (2, 5), (3, 6)]
  df = spark.createDataFrame(schema=schema, data=data)
  print(df._repr_html_())
#+end_src

#+RESULTS: pyspark-table-repr-html
#+begin_export html
<table border='1'>
<tr><th>a</th><th>b</th></tr>
<tr><td>1</td><td>4</td></tr>
<tr><td>2</td><td>5</td></tr>
<tr><td>3</td><td>6</td></tr>
</table>
#+end_export
* To plot a figure using matplotlib.pyplot
#+name: plt_show
#+BEGIN_SRC python :var fig="fig.png" :exports none :results value raw :session none
  return f'''
  plt.savefig(\"{fig}\", dpi=100.0, format="png", transparent=True)
  print(f"[[file:{fig}]]")#'''
#+END_SRC

#+BEGIN_SRC python
  import matplotlib.pyplot as plt
  import numpy as np

  x = np.linspace(0, 2 * np.pi, 100)
  y = np.sin(x)
  print("A sine wave:")
  plt.plot(x, y)
  <<plt_show("images/sine_wave.png")>>plt.show()
#+END_SRC

#+RESULTS:
:results:
A sine wave:
[[file:images/sine_wave.png]]
:end:
