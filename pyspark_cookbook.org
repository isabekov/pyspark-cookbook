#+TITLE: PySpark Cookbook
#+AUTHOR: Altynbek Isabekov
#+EMAIL: aisabekov@ku.edu.tr
#+LANGUAGE: en
#+PROPERTY: header-args:emacs-lisp :results silent
#+PROPERTY: header-args:python :noweb strip-export :exports both :results output raw :session pyspark
#+SETUPFILE: src/setupfile_html.org
* Functions :noexport:
#+name: pd2orgtbl
#+begin_src python :var df="df" :exports none :results value raw :session none
  return f"return tabulate({df}, headers={df}.columns, tablefmt='orgtbl', showindex=False)"
#+end_src

#+name: ps2orgtbl
#+begin_src python :var df_in="df_in" :exports none :results value raw :session none
  return f"print({df_in}.toPandas().fillna('null').to_markdown(index=False, tablefmt='orgtbl') + '\\n')#"
#+end_src

#+name: txtblk
#+begin_src python :var a="a" :exports none :results value raw :session none
  return f";print(\"#+begin_src text\\n\" + {a} + \"\\n#+end_src\\n\")#"
#+end_src

#+name: txtblkstart
#+begin_src python :exports none :results output raw :session none
  print("#+begin_src text");
#+end_src

#+name: txtblkend
#+begin_src python :exports none :results output raw :session none
  ;print("#+end_src")#
#+end_src

#+name: tld
#+begin_src python :exports none :results output raw :session none
  ~
#+end_src

#+name: captureoutput
#+begin_src python :exports none :results output raw :session none
  # Taken from https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call
  from io import StringIO
  import sys

  class Capturing(list):
      def __enter__(self):
          self._stdout = sys.stdout
          sys.stdout = self._stringio = StringIO()
          return self
      def __exit__(self, *args):
          self.extend(self._stringio.getvalue().splitlines())
          del self._stringio
          sys.stdout = self._stdout
#+end_src

#+name: printschema
#+begin_src python :var df="df" :exports none :results value raw :session none
  return f'''
  <<captureoutput>>
  with Capturing() as output:
      {df}.printSchema()

  # Remove trailing new line (empty string before printing)
  if output[-1] == "":
      output = output[:-1]

  print("Schema of ~{df}~ is:")
  print("#+begin_src text")
  for k in output:
      print(k)
  print("#+end_src\\n")#'''
#+end_src

#+name: prettifytable
#+begin_src python :var cmd="df.show()" :exports none :results value raw :session none
  return f'''
  <<captureoutput>>
  with Capturing() as output:
      {cmd}

  # Remove trailing new line (empty string before printing)
  if output[-1] == "":
      output = output[:-1]

  import re
  n = len(output)
  for i in range(1, n-1):
      print(re.sub("^\+([-]+)\+$",  r"|\\1|", output[i]))
  print("\\n", end="")#'''
#+end_src
* Introduction
** To create an empty dataframe
#+BEGIN_SRC python
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.StringType()), True),
          T.StructField("B", T.ArrayType(T.StringType()), True),
      ]
  )
  data = []
  df = spark.createDataFrame(schema=schema, data=data)
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| A   | B   |
|-----+-----|

** To create a dataframe with columns key and value from a dictionary
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

** To duplicate a column
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]

  df = spark.createDataFrame(values, columns)
  df = df.withColumn("value_dup", F.col("value"))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| key  | value  | value_dup |
|------+--------+-----------|
| key1 | value1 | value1    |
| key2 | value2 | value2    |

** To rename a column using ~.withColumnRenamed()~
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]

  df = spark.createDataFrame(values, columns)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()
  df = df.withColumnRenamed("key", "new_key") \
          .withColumnRenamed("value","new_value")
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Modified dataframe:
| new_key   | new_value   |
|-----------+-------------|
| key1      | value1      |
| key2      | value2      |

** To rename a column using ~.withColumnsRenamed()~
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]

  df = spark.createDataFrame(values, columns)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()
  df = df.withColumnsRenamed({"key": "new_key", "value": "new_value"})
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Modified dataframe:
| new_key   | new_value   |
|-----------+-------------|
| key1      | value1      |
| key2      | value2      |

** To rename a column using ~.select()~
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]
  df = spark.createDataFrame(values, columns)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  df = df.select(F.col("key").alias("new_key"), F.col("value").alias("new_value"))
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Modified dataframe:
| new_key   | new_value   |
|-----------+-------------|
| key1      | value1      |
| key2      | value2      |

** To rename columns by adding a prefix
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()

  schema = T.StructType(
      [
          T.StructField("index", T.IntegerType(), True),
          T.StructField("value", T.StringType(), True),
      ]
  )
  data = [(1, "Home"),
          (2, "School"),
          (3, "Home"),]
  df = spark.createDataFrame(schema=schema, data=data)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()
  print("Dataframe with renamed columns:")
  df = df.select(*[F.col(k).alias(f"prefix_{k}") for k in df.columns])
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
|   index | value   |
|---------+---------|
|       1 | Home    |
|       2 | School  |
|       3 | Home    |

Dataframe with renamed columns:
|   prefix_index | prefix_value   |
|----------------+----------------|
|              1 | Home           |
|              2 | School         |
|              3 | Home           |

** To drop columns from a dataframe
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]
  df = spark.createDataFrame(values, columns)

  df = df.withColumn("const", F.lit(1))
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  df = df.drop("value", "const")
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |   const |
|-------+---------+---------|
| key1  | value1  |       1 |
| key2  | value2  |       1 |

Modified dataframe:
| key  |
|------|
| key1 |
| key2 |

** To subset columns of a dataframe
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]
  df = spark.createDataFrame(values, columns)
  df = df.withColumn("const", F.lit(1))
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()
  print("Subset 'key', 'value' columns:")
  <<ps2orgtbl("df['key', 'value']")>>df["key", "value"].show()
  print("Subset 'key', 'const' columns:")
  <<ps2orgtbl("df['key', 'const']")>>df.select("key", "const").show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |   const |
|-------+---------+---------|
| key1  | value1  |       1 |
| key2  | value2  |       1 |

Subset 'key', 'value' columns:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Subset 'key', 'const' columns:
| key   |   const |
|-------+---------|
| key1  |       1 |
| key2  |       1 |

** To add a column with a constant value using ~F.lit()~
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]
  df = spark.createDataFrame(values, columns)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  df = df.withColumn("const_integer", F.lit(1))
  df = df.withColumn("const_string", F.lit("string"))
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Modified dataframe:
| key   | value   |   const_integer | const_string   |
|-------+---------+-----------------+----------------|
| key1  | value1  |               1 | string         |
| key2  | value2  |               1 | string         |

** To add a column with a constant value using ~.select()~
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  dict_a = {"key1": "value1", "key2": "value2"}
  values = [(k, v) for k, v in dict_a.items()]
  columns = ["key", "value"]
  df = spark.createDataFrame(values, columns)
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  df = df.select("key", "value", F.lit("const_str").alias("constant_value"))
  print("Modified dataframe:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| key   | value   |
|-------+---------|
| key1  | value1  |
| key2  | value2  |

Modified dataframe:
| key   | value   | constant_value   |
|-------+---------+------------------|
| key1  | value1  | const_str        |
| key2  | value2  | const_str        |

** To create a dataframe from a list of tuples
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
|   integer | characters   |
|-----------+--------------|
|         1 | ['A', 'B']   |
|         2 | ['C', 'D']   |
|         3 | ['E', 'F']   |

** To get the number of rows of a dataframe
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  num_rows = df.count()
  print(f"df has {num_rows} rows")
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

df has 3 rows

** To select first N rows
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  print("These are first 2 rows:")
  <<ps2orgtbl("df.limit(2)")>>df.limit(2).show()
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

These are first 2 rows:
|   integer | characters   |
|-----------+--------------|
|         1 | ['A', 'B']   |
|         2 | ['C', 'D']   |

** To deduplicate rows
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()

  schema = T.StructType(
      [
          T.StructField("key", T.IntegerType(), True),
          T.StructField("value", T.StringType(), True),
          T.StructField("comment", T.StringType(), True),
      ]
  )
  data = [(1, "Home", "a house"),
          (1, "Home", "a house"),
          (2, "School", "a building"),
          (2, "School", "a house"),
          (3, "Home", "a house"),]
  df = spark.createDataFrame(schema=schema, data=data)

  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  print("Dataframe with distinct rows:")
  <<ps2orgtbl("df.distinct()")>>df.distinct().show()

  print("Dataframe with dropped duplicate rows:")
  <<ps2orgtbl("df.dropDuplicates()")>>df.dropDuplicates().show()

  print("Dataframe with dropped duplicates in columns 'key' and 'value':")
  df = df.dropDuplicates(subset=["key", "value"])
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Original dataframe:
|   key | value   | comment    |
|-------+---------+------------|
|     1 | Home    | a house    |
|     1 | Home    | a house    |
|     2 | School  | a building |
|     2 | School  | a house    |
|     3 | Home    | a house    |

Dataframe with distinct rows:
|   key | value   | comment    |
|-------+---------+------------|
|     2 | School  | a house    |
|     3 | Home    | a house    |
|     2 | School  | a building |
|     1 | Home    | a house    |

Dataframe with dropped duplicate rows:
|   key | value   | comment    |
|-------+---------+------------|
|     2 | School  | a house    |
|     3 | Home    | a house    |
|     2 | School  | a building |
|     1 | Home    | a house    |

Dataframe with dropped duplicates in columns 'key' and 'value':
|   key | value   | comment    |
|-------+---------+------------|
|     1 | Home    | a house    |
|     2 | School  | a building |
|     3 | Home    | a house    |

** To convert a column to a list using a lambda function
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  lst = df.select("integer").rdd.map(lambda r: r[0]).collect()
  print(f"Column \"integer\" has values: <<tld>>{lst}<<tld>>")
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

Column "integer" has values: ~[1, 2, 3]~

** To convert a dataframe to a list of dictionaries corresponding to every row
#+BEGIN_SRC python
  import pprint
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  lst_dict = df.rdd.map(lambda row: row.asDict()).collect()
  print(f"Dataframe is represented as:\n")
  txt = pprint.pformat(lst_dict)<<txtblk("txt")>>
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

Dataframe is represented as:

#+begin_src text
[{'characters': ['A', 'B'], 'integer': 1},
 {'characters': ['C', 'D'], 'integer': 2},
 {'characters': ['E', 'F'], 'integer': 3}]
#+end_src

** To convert a column to a list using list comprehension
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  lst = [k["integer"] for k in df.select("integer").rdd.collect()]
  print(f"Column \"integer\" has values: <<tld>>{lst}<<tld>>")
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

Column "integer" has values: ~[1, 2, 3]~

** To convert a column to a list using Pandas
#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as F
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  values = [(1, ["A", "B"]), (2, ["C", "D"]), (3, ["E", "F"])]
  columns = ["integer", "characters"]

  df = spark.createDataFrame(values, columns)
  <<ps2orgtbl("df")>>df.show()
  lst = df.select("integer").toPandas()["integer"].tolist()
  print(f"Column \"integer\" has values: <<tld>>{lst}<<tld>>")
#+END_SRC

#+RESULTS:
| integer | characters |
|---------+------------|
|       1 | ['A', 'B'] |
|       2 | ['C', 'D'] |
|       3 | ['E', 'F'] |

Column "integer" has values: ~[1, 2, 3]~

** To display full width of a column (do not truncate)
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("sentence", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [(["A", "very", "long", "sentence"],),
          (["with", "many", "words", "."],)]
  df = spark.createDataFrame(schema=schema, data=data)

  print("Truncated output (default behavior):")
  <<prettifytable("df.show()")>>df.show()

  print("Truncated to 15 characters output:")
  <<prettifytable("df.show(truncate\07515)")>>df.show(truncate=15)

  print("Non-truncated output (show all):")
  <<prettifytable("df.show(truncate\075False)")>>df.show(truncate=False)
#+END_SRC

#+RESULTS:
Truncated output (default behavior):
|            sentence|
|--------------------|
|[A, very, long, s...|
|[with, many, word...|

Truncated to 15 characters output:
|       sentence|
|---------------|
|[A, very, lo...|
|[with, many,...|

Non-truncated output (show all):
|sentence                 |
|-------------------------|
|[A, very, long, sentence]|
|[with, many, words, .]   |

* Filtering rows
** To filter based on values of a column
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()

  schema = T.StructType(
      [
          T.StructField("Location", T.StringType(), True),
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Home", "Laptop", 12),
          ("Home", "Monitor", None),
          ("Home", "Keyboard", 9),
          ("Office", "Laptop", None),
          ("Office", "Monitor", 10),
          ("Office", "Mouse", 9)]
  df = spark.createDataFrame(schema=schema, data=data)

  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  print('Filter: <<tld>>F.col("Location" == "Home")<<tld>>')
  dft = df.filter(F.col("Location") == "Home")
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>F.col("Quantity").isNull()<<tld>>')
  dft = df.filter(F.col("Quantity").isNull())
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>F.col("Quantity").isNotNull()<<tld>>')
  dft = df.filter(F.col("Quantity").isNotNull())
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>(F.col("Location") == "Home") & (F.col("Product") == "Laptop"))<<tld>>')
  dft = df.filter((F.col("Location") == "Home") & (F.col("Product") == "Laptop"))
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>(F.col("Location") == "Home") & !(F.col("Product") == "Laptop"))<<tld>>')
  dft = df.filter((F.col("Location") == "Home") & ~(F.col("Product") == "Laptop"))
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>(F.col("Product") == "Laptop") | (F.col("Product") == "Mouse"))<<tld>>')
  dft = df.filter((F.col("Product") == "Laptop") | (F.col("Product") == "Mouse"))
  <<ps2orgtbl("dft")>>dft.show()

  print('Filter: <<tld>>F.col("Product").isin(["Laptop", "Mouse"])<<tld>>')
  dft = df.filter(F.col("Product").isin(["Laptop", "Mouse"]))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
Original dataframe:
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Laptop    | 12.0       |
| Home       | Monitor   | null       |
| Home       | Keyboard  | 9.0        |
| Office     | Laptop    | null       |
| Office     | Monitor   | 10.0       |
| Office     | Mouse     | 9.0        |

Filter: ~F.col("Location" == "Home")~
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Laptop    | 12.0       |
| Home       | Monitor   | null       |
| Home       | Keyboard  | 9.0        |

Filter: ~F.col("Quantity").isNull()~
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Monitor   | null       |
| Office     | Laptop    | null       |

Filter: ~F.col("Quantity").isNotNull()~
| Location   | Product   |   Quantity |
|------------+-----------+------------|
| Home       | Laptop    |         12 |
| Home       | Keyboard  |          9 |
| Office     | Monitor   |         10 |
| Office     | Mouse     |          9 |

Filter: ~(F.col("Location") == "Home") & (F.col("Product") == "Laptop"))~
| Location   | Product   |   Quantity |
|------------+-----------+------------|
| Home       | Laptop    |         12 |

Filter: ~(F.col("Location") == "Home") & !(F.col("Product") == "Laptop"))~
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Monitor   | null       |
| Home       | Keyboard  | 9.0        |

Filter: ~(F.col("Product") == "Laptop") | (F.col("Product") == "Mouse"))~
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Laptop    | 12.0       |
| Office     | Laptop    | null       |
| Office     | Mouse     | 9.0        |

Filter: ~F.col("Product").isin(["Laptop", "Mouse"])~
| Location   | Product   | Quantity   |
|------------+-----------+------------|
| Home       | Laptop    | 12.0       |
| Office     | Laptop    | null       |
| Office     | Mouse     | 9.0        |

* Array operations
** To create arrays of different lengths
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
          T.StructField("B", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2], [2, 3, 4, 5]),
          ([4, 5, 6], [2, 3, 4, 5])]
  df = spark.createDataFrame(schema=schema, data=data)
  dft = df.select("A", "B")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A         | B            |
|-----------+--------------|
| [1, 2]    | [2, 3, 4, 5] |
| [4, 5, 6] | [2, 3, 4, 5] |

** To calculate set difference
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.StringType()), True),
          T.StructField("B", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [(["b", "a", "c"], ["c", "d", "a", "f"])]
  df = spark.createDataFrame(schema=schema, data=data)

  dft = df.select("A", "B",
            F.array_except("A", "B").alias("A\B"),
            F.array_except("B", "A").alias("B\A"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A               | B                    | A\B   | B\A        |
|-----------------+----------------------+-------+------------|
| ['b', 'a', 'c'] | ['c', 'd', 'a', 'f'] | ['b'] | ['d', 'f'] |

** To calculate set union
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.StringType()), True),
          T.StructField("B", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [(["b", "a", "c"], ["c", "d", "a", "f"])]
  df = spark.createDataFrame(schema=schema, data=data)
  dft = df.select("A", "B",
            F.array_union("A", "B").alias("A U B"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A               | B                    | A U B                     |
|-----------------+----------------------+---------------------------|
| ['b', 'a', 'c'] | ['c', 'd', 'a', 'f'] | ['b', 'a', 'c', 'd', 'f'] |

** To calculate set intersection
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.StringType()), True),
          T.StructField("B", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [(["b", "a", "c"], ["c", "d", "a", "f"])]
  df = spark.createDataFrame(schema=schema, data=data)
  dft = df.select("A", "B", F.array_intersect("A", "B").alias("A \u2229 B"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A               | B                    | A ∩ B      |
|-----------------+----------------------+------------|
| ['b', 'a', 'c'] | ['c', 'd', 'a', 'f'] | ['a', 'c'] |

** To pad arrays with value
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
          T.StructField("B", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2], [2, 3, 4, 5]),
          ([4, 5, 6], [2, 3, 4, 5])]
  df = spark.createDataFrame(schema=schema, data=data)
  n = 4
  fill_value = 0
  df1 = df.withColumn("A_padding", F.expr(f"array_repeat({fill_value}, {n} - size(A))"))
  df1 = df1.withColumn("A_padded", F.concat("A", "A_padding"))
  dft = df1.select("A", "A_padding", "A_padded")
  <<ps2orgtbl("dft")>>dft.show()

  df2 = df.withColumn("A_padding", F.array_repeat(F.lit(fill_value), F.lit(n) - F.size("A")))
  df2 = df2.withColumn("A_padded", F.concat("A", "A_padding"))
  dft = df2.select("A", "A_padding", "A_padded")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A         | A_padding   | A_padded     |
|-----------+-------------+--------------|
| [1, 2]    | [0, 0]      | [1, 2, 0, 0] |
| [4, 5, 6] | [0]         | [4, 5, 6, 0] |

| A         | A_padding   | A_padded     |
|-----------+-------------+--------------|
| [1, 2]    | [0, 0]      | [1, 2, 0, 0] |
| [4, 5, 6] | [0]         | [4, 5, 6, 0] |

** To sum two arrays elementwise using ~F.element_at()~
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
          T.StructField("B", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2], [2, 3, 4, 5]),
          ([4, 5, 6], [2, 3, 4, 5])]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("A_padding", F.array_repeat(F.lit(fill_value), F.lit(n) - F.size("A")))
  df = df.withColumn("A_padded", F.concat("A", "A_padding"))
  df = df.withColumn("AB_sum", F.expr('transform(A_padded, (element, index) -> element + element_at(B, index + 1))'))
  dft = df.select("A", "A_padded", "B", "AB_sum")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A         | A_padded     | B            | AB_sum        |
|-----------+--------------+--------------+---------------|
| [1, 2]    | [1, 2, 0, 0] | [2, 3, 4, 5] | [3, 5, 4, 5]  |
| [4, 5, 6] | [4, 5, 6, 0] | [2, 3, 4, 5] | [6, 8, 10, 5] |

** To sum two arrays using ~F.arrays_zip()~
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
          T.StructField("B", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2], [2, 3, 4, 5]),
          ([4, 5, 6], [2, 3, 4, 5])]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("A_padding", F.array_repeat(F.lit(fill_value), F.lit(n) - F.size("A")))
  df = df.withColumn("A_padded", F.concat("A", "A_padding"))
  df = df.withColumn("AB_sum", F.expr("transform(arrays_zip(A_padded, B), x -> x.A_padded + x.B)"))
  dft = df.select("A", "A_padded", "B", "AB_sum")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A         | A_padded     | B            | AB_sum        |
|-----------+--------------+--------------+---------------|
| [1, 2]    | [1, 2, 0, 0] | [2, 3, 4, 5] | [3, 5, 4, 5]  |
| [4, 5, 6] | [4, 5, 6, 0] | [2, 3, 4, 5] | [6, 8, 10, 5] |

** To find mode of an array (most common element)
#+BEGIN_SRC python
  from collections import Counter
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2, 2, 4],),
          ([4, 5, 6, 7],),
          ([1, 1, 2, 2],)]
  df = spark.createDataFrame(schema=schema, data=data)

  @F.udf
  def udf_mode(x):
      return Counter(x).most_common(1)[0][0]

  dft = df.withColumn("mode", udf_mode("A"))
  <<printschema("dft")>>dft.printSchema()
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
Schema of ~dft~ is:
#+begin_src text
root
 |-- A: array (nullable = true)
 |    |-- element: integer (containsNull = true)
 |-- mode: string (nullable = true)
#+end_src

| A            |   mode |
|--------------+--------|
| [1, 2, 2, 4] |      2 |
| [4, 5, 6, 7] |      4 |
| [1, 1, 2, 2] |      1 |

** To calculate difference of two consecutive elements in an array
#+BEGIN_SRC python
  import numpy as np
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("id", T.StringType(), True),
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [("A", [4, 1, 0, 2]),
          ("B", [1, 0, 3, 1])]
  df = spark.createDataFrame(schema=schema, data=data)

  @F.udf(returnType=T.ArrayType(T.IntegerType()))
  def diff_of_two_consecutive_elements(x):
      return np.ediff1d(np.array(x)).tolist()

  df = df.withColumn("diff", diff_of_two_consecutive_elements(F.col("values")))
  <<ps2orgtbl("df")>>df.show()
  <<printschema("df")>>df.printSchema()
#+END_SRC

#+RESULTS:
| id | values       | diff        |
|----+--------------+-------------|
| A  | [4, 1, 0, 2] | [-3, -1, 2] |
| B  | [1, 0, 3, 1] | [-1, 3, -2] |

Schema of ~df~ is:
#+begin_src text
root
 |-- id: string (nullable = true)
 |-- values: array (nullable = true)
 |    |-- element: integer (containsNull = true)
 |-- diff: array (nullable = true)
 |    |-- element: integer (containsNull = true)
#+end_src

** To apply a function to every element of an array
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("words_with_suffixes", T.ArrayType(T.StringType()), True)
      ]
  )
  data = [(["pen_10", "note_11", "bottle_12"],), (["apple_13", "orange_14", "lemon_15"],),]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("words", F.transform("words_with_suffixes", lambda x: F.split(x, "_").getItem(0)))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| words_with_suffixes                   | words                        |
|---------------------------------------+------------------------------|
| ['pen_10', 'note_11', 'bottle_12']    | ['pen', 'note', 'bottle']    |
| ['apple_13', 'orange_14', 'lemon_15'] | ['apple', 'orange', 'lemon'] |

** To deduplicate elements in an array (find unique/distinct elements)
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("words", T.ArrayType(T.StringType()), True)
      ]
  )
  data = [(["pen", "note", "pen"],), (["apple", "apple", "lemon"],),]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("unique_words", F.array_distinct("words"))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| words                       | unique_words       |
|-----------------------------+--------------------|
| ['pen', 'note', 'pen']      | ['pen', 'note']    |
| ['apple', 'apple', 'lemon'] | ['apple', 'lemon'] |

** To create a map (dictionary) from two arrays (one with keys, one with values)
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local[1]").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("keys", T.ArrayType(T.IntegerType()), True),
          T.StructField("values", T.ArrayType(T.StringType()), True),
      ]
  )
  data = [([1, 2, 3], ["A", "B", "C"])]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("map_kv", F.map_from_arrays("keys", "values"))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| keys      | values          | map_kv                   |
|-----------+-----------------+--------------------------|
| [1, 2, 3] | ['A', 'B', 'C'] | {1: 'A', 2: 'B', 3: 'C'} |

** To calculate mean of an array
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2],),
          ([4, 5, 6],)]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("mean", F.aggregate(
            "values",                                  # column
            F.lit(0),                                  # initialValue
            lambda acc, x: acc + x,                    # merge operation
            lambda acc: acc / F.size(F.col("values")), # finish
        ))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values    |   mean |
|-----------+--------|
| [1, 2]    |    1.5 |
| [4, 5, 6] |    5   |

** To find out whether an array has any negative elements
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, -2],),
          ([4, 5, 6],)]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("any_negative", F.exists("values", lambda x: x < 0))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values    | any_negative |
|-----------+--------------|
| [1, -2]   | True         |
| [4, 5, 6] | False        |

** To convert elements of an array to columns
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("A", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2, 3, 4],),
          ([5, 6, 7],)]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("first", F.col("A").getItem(0))
  dft = df.select("A", "first", *[F.col("A").getItem(k).alias(f"element_{k+1}") for k in range(1,4)])
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| A            | first | element_2 | element_3 | element_4 |
|--------------+-------+-----------+-----------+-----------|
| [1, 2, 3, 4] |     1 |         2 |         3 |         4 |
| [5, 6, 7]    |     5 |         6 |         7 |       nan |

** To find location of the first occurence of an element in an array
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  import pandas as pd
  from pyspark.sql import SparkSession
  import numpy as np
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 7, 5],),
          ([7, 4, 7],)]
  df = spark.createDataFrame(schema=schema, data=data)

  df = df.withColumn("position", F.array_position(F.col("values"), 7))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values    | position |
|-----------+----------|
| [1, 7, 5] |        2 |
| [7, 4, 7] |        1 |

** To calculate moving difference of two consecutive elements in an array
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  import pandas as pd
  from pyspark.sql import SparkSession
  import numpy as np
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 2, 5],),
          ([4, 4, 6],)]
  df = spark.createDataFrame(schema=schema, data=data)

  @F.pandas_udf(T.ArrayType(T.IntegerType()))
  def diff2e(x: pd.Series) -> pd.Series:
      return x.apply(lambda x: (x[1:] - x[:-1]))

  @F.udf(returnType=T.ArrayType(T.IntegerType()))
  def diff_of_two_consecutive_elements(x):
      return np.ediff1d(np.array(x)).tolist()

  df = df.withColumn("diff2e", diff2e(F.col("values")))
  df = df.withColumn("ediff1d", diff_of_two_consecutive_elements(F.col("values")))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values    | diff2e   | ediff1d   |
|-----------+----------+-----------|
| [1, 2, 5] | [1, 3]   | [1, 3]    |
| [4, 4, 6] | [0, 2]   | [0, 2]    |

** To slice an array
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  import pandas as pd
  from pyspark.sql import SparkSession
  import numpy as np
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 7, 5, 2],),
          ([6, 4, 7, 3],)]
  df = spark.createDataFrame(schema=schema, data=data)

  df = df.withColumn("values[1:3]", F.slice("values", start=2, length=2))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values       | values[1:3] |
|--------------+-------------|
| [1, 7, 5, 2] | [7, 5]      |
| [6, 4, 7, 3] | [4, 7]      |

** To slice an array dynamically
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  import pandas as pd
  from pyspark.sql import SparkSession
  import numpy as np
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("values", T.ArrayType(T.IntegerType()), True),
      ]
  )
  data = [([1, 7, 5],),
          ([6, 4, 7, 3],)]
  df = spark.createDataFrame(schema=schema, data=data)
  start_idx = 2
  df = df.withColumn("values[1:]", F.slice("values", start=2, length=(F.size("values") - F.lit(start_idx - 1))))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| values       | values[1:] |
|--------------+------------|
| [1, 7, 5]    | [7, 5]     |
| [6, 4, 7, 3] | [4, 7, 3]  |

* Text processing
** To remove prefix from a string using a UDF
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("text", T.StringType(), True),
      ]
  )
  data = [("id_orange",),
          ("apple",)]
  df = spark.createDataFrame(schema=schema, data=data)
  remove_prefix = F.udf(lambda x: x[3:] if x[:3] == "id_" else x, T.StringType())
  df = df.withColumn("no_prefix", remove_prefix(F.col("text")))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| text      | no_prefix |
|-----------+-----------|
| id_orange | orange    |
| apple     | apple     |

** To split a string into letters (characters) using regex
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("String", T.StringType(), True)
      ]
  )
  data = [["This is"]]
  df = spark.createDataFrame(schema=schema, data=data)
  dft = df.select('String', F.split('String', '(?!$)').alias("Characters"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| String  | Characters                          |
|---------+-------------------------------------|
| This is | ['T', 'h', 'i', 's', ' ', 'i', 's'] |

** To concatenate columns with strings using a separator
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Str1", T.StringType(), True),
          T.StructField("Str2", T.StringType(), True)
      ]
  )
  data = [("This is", "a string"),
          ("on a", "different row")]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("Str_Concat", F.concat_ws( "_", "Str1", "Str2"))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| Str1    | Str2          | Str_Concat         |
|---------+---------------+--------------------|
| This is | a string      | This is_a string   |
| on a    | different row | on a_different row |

** To split a string into letters (characters) using split function
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("String", T.StringType(), True)
      ]
  )
  data = [["This is"]]
  df = spark.createDataFrame(schema=schema, data=data)
  fsplit = F.expr("split(String, '')")
  dft = df.select('String', fsplit.alias("Characters"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| String  | Characters                          |
|---------+-------------------------------------|
| This is | ['T', 'h', 'i', 's', ' ', 'i', 's'] |

** To split a string into letters (characters) and remove last character
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("String", T.StringType(), True)
      ]
  )
  data = [["This is_"]]
  df = spark.createDataFrame(schema=schema, data=data)
  print("Using split function and remove last character:")
  fsplit = "split(String, '')"
  fsplit = F.expr(f'slice({fsplit}, 1, size({fsplit}) - 1)')
  dft = df.select('String', fsplit.alias("Characters"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
Using split function and remove last character:
| String   | Characters                          |
|----------+-------------------------------------|
| This is_ | ['T', 'h', 'i', 's', ' ', 'i', 's'] |

** To append a string to all values in a column
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Str1", T.StringType(), True),
          T.StructField("Str2", T.StringType(), True)
      ]
  )
  data = [("This is", "a string"),
          ("on a", "different row")]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("Str1_with_prefix", F.concat(F.lit("Prefix_"), "Str1"))
  dft = df.select("Str1", "Str1_with_prefix")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| Str1    | Str1_with_prefix |
|---------+------------------|
| This is | Prefix_This is   |
| on a    | Prefix_on a      |

* Time operations
** To calculate cumulative sum of a column
#+BEGIN_SRC python
  import pandas as pd
  from pyspark.sql import Window
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  df = pd.DataFrame({'time': [0, 1, 2, 3, 4, 5],
                     'value': [False, False, True, False, True, True]})

  df = spark.createDataFrame(df)
  df = df.withColumn("cml_n_true", F.sum((F.col("value") == True).cast("int")).over(Window.orderBy(F.col("time").asc())))
  df = df.withColumn("cml_n_false", F.sum((F.col("value") == False).cast("int")).over(Window.orderBy(F.col("time").asc())))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| time | value | cml_n_true | cml_n_false |
|------+-------+------------+-------------|
|    0 | False |          0 |           1 |
|    1 | False |          0 |           2 |
|    2 | True  |          1 |           2 |
|    3 | False |          1 |           3 |
|    4 | True  |          2 |           3 |
|    5 | True  |          3 |           3 |

** To convert Unix time stamp to human readable format
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("timestamp", T.LongType(), True),
      ]
  )
  data = [(1703224755,),
          (1703285602,)]
  df = spark.createDataFrame(schema=schema, data=data)

  df = df.withColumn("time_stamp_hrf", F.from_unixtime(F.col("timestamp")))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
|  timestamp | time_stamp_hrf      |
|------------+---------------------|
| 1703224755 | 2023-12-22 06:59:15 |
| 1703285602 | 2023-12-22 23:53:22 |

* Numerical operations
** To find percentage of a column
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Laptop", 12),
          ("Monitor", 7),
          ("Mouse", 8),
          ("Keyboard", 9)]
  df = spark.createDataFrame(schema=schema, data=data)

  df = df.withColumn("%", F.round(F.col("Quantity")/F.sum("Quantity").over(Window.partitionBy())*100, 2))
  dft = df.select("Product", "Quantity", "%").orderBy(F.desc("Quantity"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| Product   |   Quantity |     % |
|-----------+------------+-------|
| Laptop    |         12 | 33.33 |
| Keyboard  |          9 | 25    |
| Mouse     |          8 | 22.22 |
| Monitor   |          7 | 19.44 |

** To find percentage of a column within a group using a window
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Location", T.StringType(), True),
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Home", "Laptop", 12),
          ("Home", "Monitor", 7),
          ("Home", "Mouse", 8),
          ("Home", "Keyboard", 9),
          ("Office", "Laptop", 23),
          ("Office", "Monitor", 10),
          ("Office", "Mouse", 9)]
  df = spark.createDataFrame(schema=schema, data=data)

  df = df.withColumn("%", F.round(F.col("Quantity")/F.sum("Quantity").over(Window.partitionBy("Location"))*100, 2))
  dft = df.select("Location", "Product", "Quantity", "%").orderBy(F.desc("Location"), F.desc("Quantity"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| Location | Product  | Quantity |     % |
|----------+----------+----------+-------|
| Office   | Laptop   |       23 | 54.76 |
| Office   | Monitor  |       10 | 23.81 |
| Office   | Mouse    |        9 | 21.43 |
| Home     | Laptop   |       12 | 33.33 |
| Home     | Keyboard |        9 |    25 |
| Home     | Mouse    |        8 | 22.22 |
| Home     | Monitor  |        7 | 19.44 |

** To find percentage of a column within a group using ~.groupBy()~ and a join
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Location", T.StringType(), True),
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Home", "Laptop", 12),
          ("Home", "Monitor", 7),
          ("Home", "Mouse", 8),
          ("Home", "Keyboard", 9),
          ("Office", "Laptop", 23),
          ("Office", "Monitor", 10),
          ("Office", "Mouse", 9)]
  df = spark.createDataFrame(schema=schema, data=data)

  df_sum = df.groupBy("Location").agg(F.sum("Quantity").alias("Total_Quantity"))
  df = df.join(df_sum, on="Location").withColumn("%", F.round(F.col("Quantity")/F.col("Total_Quantity")*100, 2))
  dft = df.select("Location", "Product", "Quantity", "%").orderBy(F.desc("Location"), F.desc("Quantity"))
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
| Location | Product  | Quantity |     % |
|----------+----------+----------+-------|
| Office   | Laptop   |       23 | 54.76 |
| Office   | Monitor  |       10 | 23.81 |
| Office   | Mouse    |        9 | 21.43 |
| Home     | Laptop   |       12 | 33.33 |
| Home     | Keyboard |        9 |    25 |
| Home     | Mouse    |        8 | 22.22 |
| Home     | Monitor  |        7 | 19.44 |

** To find maximum value of a column
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Location", T.StringType(), True),
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Home", "Laptop", 12),
          ("Home", "Monitor", 7),
          ("Home", "Mouse", 8),
          ("Home", "Keyboard", 9),
          ("Office", "Laptop", 23),
          ("Office", "Monitor", 10),
          ("Office", "Mouse", 9)]
  df = spark.createDataFrame(schema=schema, data=data)
  <<ps2orgtbl("df")>>df.show()
  max_val = df.select("Quantity").rdd.max()[0]
  print(f"Maximum value of Quantity: {max_val}")
#+END_SRC

#+RESULTS:
| Location | Product  | Quantity |
|----------+----------+----------|
| Home     | Laptop   |       12 |
| Home     | Monitor  |        7 |
| Home     | Mouse    |        8 |
| Home     | Keyboard |        9 |
| Office   | Laptop   |       23 |
| Office   | Monitor  |       10 |
| Office   | Mouse    |        9 |

Maximum value of Quantity: 23

** To add a column with count of elements per group
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Location", T.StringType(), True),
          T.StructField("Product", T.StringType(), True),
          T.StructField("Quantity", T.IntegerType(), True),
      ]
  )
  data = [("Home", "Laptop", 12),
          ("Home", "Monitor", 7),
          ("Home", "Mouse", 8),
          ("Home", "Keyboard", 9),
          ("Office", "Laptop", 23),
          ("Office", "Monitor", 10),
          ("Office", "Mouse", 9)]
  df = spark.createDataFrame(schema=schema, data=data)
  df = df.withColumn("count_per_group", F.count(F.lit(1)).over(Window.partitionBy(F.col("Location"))))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
| Location | Product  | Quantity | count_per_group |
|----------+----------+----------+-----------------|
| Home     | Laptop   |       12 |               4 |
| Home     | Monitor  |        7 |               4 |
| Home     | Mouse    |        8 |               4 |
| Home     | Keyboard |        9 |               4 |
| Office   | Laptop   |       23 |               3 |
| Office   | Monitor  |       10 |               3 |
| Office   | Mouse    |        9 |               3 |

* Dataframe join operations
** To perform a full, outer, left, right join operations
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Name", T.StringType(), True),
          T.StructField("Score", T.IntegerType(), True),
      ]
  )
  data = [("Alice", 10),
          ("Bob", 11)
          ]
  df_a = spark.createDataFrame(schema=schema, data=data)
  print("Table A:")
  <<ps2orgtbl("df_a")>>df_a.show()

  schema = T.StructType(
      [
          T.StructField("Name", T.StringType(), True),
          T.StructField("Height", T.StringType(), True),
      ]
  )
  data = [("Alice", 12),
          ("Jane", 7),
  ]
  df_b = spark.createDataFrame(schema=schema, data=data)
  print("Table B:")
  <<ps2orgtbl("df_b")>>df_b.show()

  df = df_a.join(df_b, on="name", how="full")
  print("Full join on 'name':")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, on="name", how="outer")
  print("Outer join on 'name':")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, df_a["Name"] == df_b["Name"])
  print("Join on 'name' on equal condition:")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, on="name", how="inner")
  print("Inner join on 'name':")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, on="name", how="left")
  print("Left join on 'name':")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, on="name", how="right")
  print("Right join on 'name':")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Table A:
| Name  | Score |
|-------+-------|
| Alice |    10 |
| Bob   |    11 |

Table B:
| Name   |   Height |
|--------+----------|
| Alice  |       12 |
| Jane   |        7 |

Full join on 'name':
| Name   | Score   | Height   |
|--------+---------+----------|
| Alice  | 10.0    | 12       |
| Bob    | 11.0    | null     |
| Jane   | null    | 7        |

Outer join on 'name':
| Name   | Score   | Height   |
|--------+---------+----------|
| Alice  | 10.0    | 12       |
| Bob    | 11.0    | null     |
| Jane   | null    | 7        |

Join on 'name' on equal condition:
| Name  | Score | Name  | Height |
|-------+-------+-------+--------|
| Alice |    10 | Alice |     12 |

Inner join on 'name':
| Name  | Score | Height |
|-------+-------+--------|
| Alice |    10 |     12 |

Left join on 'name':
| Name   |   Score | Height   |
|--------+---------+----------|
| Bob    |      11 | null     |
| Alice  |      10 | 12       |

Right join on 'name':
| Name   | Score   |   Height |
|--------+---------+----------|
| Alice  | 10.0    |       12 |
| Jane   | null    |        7 |

** To drop one of the duplicate columns after join
#+BEGIN_SRC python
  from pyspark.sql import Row
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  df_a = spark.createDataFrame([
    Row(id=1, value="A1"),
    Row(id=1, value="B1"),
    Row(id=1, value="C1"),
    Row(id=2, value="A1"),
    Row(id=2, value="X1"),
    Row(id=2, value="Y1")]
  )
  print("Dataframe <<tld>>df_a<<tld>>:")
  <<ps2orgtbl("df_a")>>df_1.show()

  df_b = spark.createDataFrame([
    Row(id=1, updated="A2"),
    Row(id=1, updated="B1"),
    Row(id=1, updated="C1"),
    Row(id=2, updated="A1"),
    Row(id=2, updated="X1"),
    Row(id=2, updated="Y1")]
  )
  print("Dataframe <<tld>>df_b<<tld>>:")
  <<ps2orgtbl("df_b")>>df_2.show()

  df = df_a.join(df_b, on=[df_a["id"] == df_b["id"], df_a["value"] == df_b["updated"]], how="full")
  print("Full join on <<tld>>df_a['value'] == df_b['updated']<<tld>>:")
  <<ps2orgtbl("df")>>df.show()

  df = df_a.join(df_b, on=[df_a["id"] == df_b["id"], df_a["value"] == df_b["updated"]], how="full").drop(df_b["id"])
  print("Full join on <<tld>>df_a['value'] == df_b['updated']<<tld>> with dropped <<tld>>df_b['id']<<tld>> column:")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:
Dataframe ~df_a~:
|   id | value   |
|------+---------|
|    1 | A1      |
|    1 | B1      |
|    1 | C1      |
|    2 | A1      |
|    2 | X1      |
|    2 | Y1      |

Dataframe ~df_b~:
|   id | updated   |
|------+-----------|
|    1 | A2        |
|    1 | B1        |
|    1 | C1        |
|    2 | A1        |
|    2 | X1        |
|    2 | Y1        |

Full join on ~df_a['value'] == df_b['updated']~:
| id   | value   | id   | updated   |
|------+---------+------+-----------|
| 1.0  | A1      | null | null      |
| null | null    | 1.0  | A2        |
| 1.0  | B1      | 1.0  | B1        |
| 1.0  | C1      | 1.0  | C1        |
| 2.0  | A1      | 2.0  | A1        |
| 2.0  | X1      | 2.0  | X1        |
| 2.0  | Y1      | 2.0  | Y1        |

Full join on ~df_a['value'] == df_b['updated']~ with dropped ~df_b['id']~ column:
| id   | value   | updated   |
|------+---------+-----------|
| 1.0  | A1      | null      |
| null | null    | A2        |
| 1.0  | B1      | B1        |
| 1.0  | C1      | C1        |
| 2.0  | A1      | A1        |
| 2.0  | X1      | X1        |
| 2.0  | Y1      | Y1        |

* Aggregation and maps
** To group by and aggregate into a map using ~F.map_from_entries()~
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  from pyspark.sql import Row
  from pyspark.sql.window import Window
  from pyspark.sql import SparkSession

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  df = spark.createDataFrame([
    Row(id=1, key='a', value="A1"),
    Row(id=1, key='b', value="B1"),
    Row(id=1, key='c', value="C1"),
    Row(id=2, key='a', value="A1"),
    Row(id=2, key='x', value="X1"),
    Row(id=2, key='y', value="Y1")]
  )

  print("Dataframe with keys and values:")
  <<ps2orgtbl("df")>>df.show()
  dft = df.groupBy("id").agg(F.map_from_entries(F.collect_list(
            F.struct("key", "value"))).alias("key_value")
  )
  print("Dataframe with key -> value mapping")
  <<ps2orgtbl("dft")>>dft.show()
  <<printschema("dft")>>dft.printSchema()
#+END_SRC

#+RESULTS:
Dataframe with keys and values:
|   id | key   | value   |
|------+-------+---------|
|    1 | a     | A1      |
|    1 | b     | B1      |
|    1 | c     | C1      |
|    2 | a     | A1      |
|    2 | x     | X1      |
|    2 | y     | Y1      |

Dataframe with key -> value mapping
|   id | key_value                         |
|------+-----------------------------------|
|    1 | {'a': 'A1', 'b': 'B1', 'c': 'C1'} |
|    2 | {'x': 'X1', 'a': 'A1', 'y': 'Y1'} |

Schema of ~dft~ is:
#+begin_src text
root
 |-- id: long (nullable = true)
 |-- key_value: map (nullable = false)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)
#+end_src

** To group by and aggregate into a map using UDF
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  from pyspark.sql import Row
  from pyspark.sql import SparkSession
  import pyspark.sql.types as T

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  df = spark.createDataFrame([
    Row(id=1, key='a', value="A1"),
    Row(id=1, key='b', value="B1"),
    Row(id=1, key='c', value="C1"),
    Row(id=2, key='a', value="A1"),
    Row(id=2, key='x', value="X1"),
    Row(id=2, key='y', value="Y1")]
  )

  print("Dataframe with keys and values:")
  <<ps2orgtbl("df")>>df.show()

  @F.udf(returnType=T.MapType(T.StringType(), T.StringType()))
  def map_array(column):
      return dict(column)

  dft = (df.groupBy("id")
     .agg(F.collect_list(F.struct("key", "value")).alias("key_value"))
     .withColumn('key_value', map_array('key_value')))
  print("Dataframe with keys and values:")
  <<ps2orgtbl("dft")>>dft.show()
  <<printschema("dft")>>dft.printSchema()
#+END_SRC

#+RESULTS:
Dataframe with keys and values:
|   id | key   | value   |
|------+-------+---------|
|    1 | a     | A1      |
|    1 | b     | B1      |
|    1 | c     | C1      |
|    2 | a     | A1      |
|    2 | x     | X1      |
|    2 | y     | Y1      |

Dataframe with keys and values:
|   id | key_value                         |
|------+-----------------------------------|
|    1 | {'a': 'A1', 'b': 'B1', 'c': 'C1'} |
|    2 | {'x': 'X1', 'a': 'A1', 'y': 'Y1'} |

Schema of ~dft~ is:
#+begin_src text
root
 |-- id: long (nullable = true)
 |-- key_value: map (nullable = true)
 |    |-- key: string
 |    |-- value: string (valueContainsNull = true)
#+end_src

** To agregate over multiple columns and sum values of dictionaries
#+BEGIN_SRC python
  import pyspark.sql.types as T
  import pyspark.sql.functions as F
  from pyspark.sql import SparkSession

  df_schema = T.StructType([T.StructField('clid', T.StringType(), True),
                          T.StructField('coef_1', T.MapType(T.StringType(), T.DoubleType(), True), False),
                          T.StructField('coef_2', T.MapType(T.StringType(), T.DoubleType(), True), False),
                          T.StructField('coef_3', T.MapType(T.StringType(), T.DoubleType(), True), False)])
  df_data = [["X", {'B': 0.4, 'C': 0.4}, {'B': 0.33, 'C': 0.5}, {'A': 0.5, 'C': 0.33}],
             ["Y", {'B': 0.67, 'C': 0.33}, {'B': 0.85}, {'A': 0.4, 'C': 0.57}],
             ]
  spark = SparkSession.builder\
          .appName("Parse DataFrame Schema")\
          .getOrCreate()
  df = spark.createDataFrame(data=df_data, schema=df_schema)

  df = df.withColumn("coef_total", F.col("coef_1"))
  for i in range(2,4):
      df = df.withColumn("coef_total", F.map_zip_with("coef_total", f"coef_{i}",
                        lambda k, v1, v2: F.when(v1.isNull(), 0).otherwise(v1) + F.when(v2.isNull(), 0).otherwise(v2)))
  <<ps2orgtbl("df")>>df.show()
#+END_SRC

#+RESULTS:

| clid | coef_1                 | coef_2                | coef_3                | coef_total                                     |
|------+------------------------+-----------------------+-----------------------+------------------------------------------------|
| X    | {'B': 0.4, 'C': 0.4}   | {'B': 0.33, 'C': 0.5} | {'A': 0.5, 'C': 0.33} | {'A': 0.5, 'B': 0.73, 'C': 1.23}               |
| Y    | {'B': 0.67, 'C': 0.33} | {'B': 0.85}           | {'A': 0.4, 'C': 0.57} | {'A': 0.4, 'B': 1.52, 'C': 0.8999999999999999} |

* Sampling rows
** To sample rows
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("index", T.IntegerType(), True),
          T.StructField("value", T.StringType(), True),
      ]
  )
  data = [(1, "Home"),
          (2, "School"),
          (3, "Home"),
          (4, "Home"),
          (5, "Office"),
          (6, "Office"),
          (7, "Office"),
          (8, "Mall"),
          (9, "Mall"),
          (10, "School")]
  df = spark.createDataFrame(schema=schema, data=data).repartition(3)
  df = df.withColumn("partition", F.spark_partition_id()).orderBy("index")
  print("Original dataframe:")
  <<ps2orgtbl("df")>>df.show()

  print("Sampled dataframe:")
  dft = df.sample(fraction=0.5, seed=1).orderBy("index")
  <<ps2orgtbl("dft")>>dft.show()
#+END_SRC

#+RESULTS:
Original dataframe:
|   index | value   |   partition |
|---------+---------+-------------|
|       1 | Home    |           1 |
|       2 | School  |           0 |
|       3 | Home    |           0 |
|       4 | Home    |           2 |
|       5 | Office  |           2 |
|       6 | Office  |           2 |
|       7 | Office  |           1 |
|       8 | Mall    |           0 |
|       9 | Mall    |           1 |
|      10 | School  |           0 |

Sampled dataframe:
|   index | value   |   partition |
|---------+---------+-------------|
|       3 | Home    |           0 |
|       4 | Home    |           2 |
|       7 | Office  |           1 |
|       8 | Mall    |           0 |
|       9 | Mall    |           1 |

* UUID generation
** To generate a UUID for every row
#+BEGIN_SRC python
  import pyspark.sql.functions as F
  import pyspark.sql.types as T
  from pyspark.sql import SparkSession
  import random
  import uuid

  spark = SparkSession.builder.master("local").appName("test-app").getOrCreate()
  schema = T.StructType(
      [
          T.StructField("Name", T.StringType(), True),
      ]
  )
  data = [["Alice"],
          ["Bon"],
          ["John"],
          ["Cecile"]
          ]
  df = spark.createDataFrame(schema=schema, data=data).repartition(2)

  def _generate_uuid(uuid_gen, v=10):
      def _replace_byte(value: int, byte: int):
          byte = byte & 0xF
          bit_shift = 76
          mask = ~(0xF << bit_shift)
          return value & mask | (byte << bit_shift)

      uuid_ = uuid_gen.generate()
      return uuid.UUID(int=(_replace_byte(uuid_.int, v)))

  class RandomDistributedUUIDGenerator:
      def generate(self):
          return uuid.uuid4()

  class SeedBasedUUIDGenerator:
      def __init__(self, seed):
          self.rnd = random.Random(seed)

      def generate(self):
          return uuid.UUID(int=self.rnd.getrandbits(128), version=4)

  gen = RandomDistributedUUIDGenerator()
  udf_generate_uuid = F.udf(lambda: _generate_uuid(gen).__str__(), T.StringType())
  df = df.withColumn("UUID_random_distributed", udf_generate_uuid())

  seed_for_rng = 1
  gen = SeedBasedUUIDGenerator(seed_for_rng)
  udf_generate_uuid = F.udf(lambda: _generate_uuid(gen).__str__(), T.StringType())
  df = df.withColumn("UUID_seed_based", udf_generate_uuid())

  print("The dataframe resides in two partitions. Seed-based random UUID generator uses the same seed on both partitions, yielding identical values.")
  <<ps2orgtbl("df")>>df.show()
#+END_SRC
#+RESULTS:
The dataframe resides in two partitions. Seed-based random UUID generator uses the same seed on both partitions, yielding identical values.
| Name   | UUID_random_distributed              | UUID_seed_based                      |
|--------+--------------------------------------+--------------------------------------|
| John   | c9f3c8c8-52fd-a014-998c-a520edf7510e | cd613e30-d8f1-aadf-91b7-584a2265b1f5 |
| Bon    | 2ce5c2ba-80af-ade9-b93e-03d5d4629ef1 | 1e2feb89-414c-a43c-9027-c4d1c386bbc4 |
| Cecile | 0289eb79-6535-a019-89aa-5f8227c204ac | cd613e30-d8f1-aadf-91b7-584a2265b1f5 |
| Alice  | 4e9a1383-7949-a07e-8302-f6cf53020a58 | 1e2feb89-414c-a43c-9027-c4d1c386bbc4 |
